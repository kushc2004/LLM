{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlU/nGuJVDdkSW/TyozfKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushc2004/LLM/blob/main/programmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.38.0"
      ],
      "metadata": {
        "id": "TVftwllSgE05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, GemmaTokenizer\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model"
      ],
      "metadata": {
        "id": "BDFFsFm-gd28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Kl3yAnbeg9kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6nxWJjNf0mI"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, name, description, model_name, **kwargs):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.system_prompt = \"You're a helpful assistant.\"\n",
        "        self.kwargs = kwargs\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Load the tokenizer\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=os.environ['HF_TOKEN'])\n",
        "\n",
        "        # Initialize the model with empty weights and use `init_empty_weights`\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0},\n",
        "                                             token=os.environ['HF_TOKEN'])\n",
        "\n",
        "        # Infer the device map\n",
        "        # device_map = infer_auto_device_map(self.model, max_memory={0: \"12GiB\", \"cpu\": \"30GiB\"})\n",
        "\n",
        "        # Dispatch the model to the appropriate devices\n",
        "        # self.model = dispatch_model(self.model, device_map=device_map)\n",
        "\n",
        "    def llm_call(\n",
        "        self,\n",
        "        prompt: Optional[str] = None,\n",
        "        messages: Optional[List] = None,\n",
        "        seed: int = 10,\n",
        "    ) -> str:\n",
        "        # Ensure exactly one of prompt or messages is provided\n",
        "        assert (prompt is None) != (messages is None)\n",
        "\n",
        "        # Ensure if messages is provided, it is a list of dicts with role and content\n",
        "        if messages is not None:\n",
        "            assert isinstance(messages, list)\n",
        "            for message in messages:\n",
        "                assert isinstance(message, dict)\n",
        "                assert \"role\" in message\n",
        "                assert \"content\" in message\n",
        "\n",
        "        if prompt is not None:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "\n",
        "        # Concatenate messages into a single prompt\n",
        "        full_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
        "\n",
        "        # Tokenize the input\n",
        "        device = \"cuda:0\"\n",
        "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate response\n",
        "        # with torch.no_grad():\n",
        "            # outputs = self.model.generate(**inputs, max_length=200, do_sample=True, top_p=0.95, top_k=50, temperature=0.7)\n",
        "\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=20)\n",
        "        print(\"output generated\")\n",
        "\n",
        "        # Decode the response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def generate_reply(\n",
        "        self,\n",
        "        task: str,\n",
        "        state: Dict,\n",
        "        sender: \"Agent\",\n",
        "    ) -> Tuple[str, Dict]:\n",
        "        return (\n",
        "            \"This is a reply from the agent. REPLY NOT IMPLEMENTED! Terminate the whole process!\",\n",
        "            state,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "class Agent1:\n",
        "    def __init__(self, name, description, model_name, **kwargs):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.system_prompt = \"You're a helpful assistant.\"\n",
        "        self.kwargs = kwargs\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Load the tokenizer\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=os.environ['HF_TOKEN'])\n",
        "\n",
        "        # Initialize the model with empty weights and use `init_empty_weights`\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\": 0},\n",
        "                                             token=os.environ['HF_TOKEN'])\n",
        "\n",
        "    def llm_call(self, prompt: Optional[str] = None, messages: Optional[List] = None, seed: int = 10) -> str:\n",
        "        # Ensure exactly one of prompt or messages is provided\n",
        "        assert (prompt is None) != (messages is None)\n",
        "\n",
        "        # Ensure if messages are provided, it is a list of dicts with role and content\n",
        "        if messages is not None:\n",
        "            assert isinstance(messages, list)\n",
        "            for message in messages:\n",
        "                assert isinstance(message, dict)\n",
        "                assert \"role\" in message\n",
        "                assert \"content\" in message\n",
        "\n",
        "        if prompt is not None:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "\n",
        "        # Concatenate messages into a single prompt\n",
        "        full_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
        "\n",
        "        # Tokenize the input\n",
        "        device = \"cuda:0\"\n",
        "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate response\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=50,\n",
        "                                      num_return_sequences=1,\n",
        "                                      pad_token_id=self.tokenizer.eos_token_id,\n",
        "                                      )\n",
        "        print(\"output generated\")\n",
        "\n",
        "        # Decode the response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "        print(\"output decoded\")\n",
        "        print(response,\"\\n\\n\")\n",
        "        # Extract only the assistant's response\n",
        "        response_lines = response.split(\"\\n\")\n",
        "        assistant_response = next((line for line in response_lines if line.startswith(\"assistant:\")), \"\")\n",
        "        assistant_response = assistant_response[len(\"assistant:\"):].strip()\n",
        "\n",
        "        print(assistant_response)\n",
        "        return assistant_response\n",
        "\n",
        "    def generate_reply(self, task: str, state: Dict, sender: \"Agent\") -> Tuple[str, Dict]:\n",
        "        return (\n",
        "            \"This is a reply from the agent. REPLY NOT IMPLEMENTED! Terminate the whole process!\",\n",
        "            state,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "3X8DT28BjmPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(name=\"TestAgent\", description=\"A test agent\", model_name=\"google/gemma-2b\")\n",
        "\n",
        "# Call the model with a prompt\n",
        "response = agent.llm_call(prompt='''\n",
        "Assume the parameters are defined. Now generate a code accordingly and enclose it between \"=====\" lines. Only generate the gurobi code, and don't generate any other text. Here's an example:\n",
        "\n",
        "**input**:\n",
        "\n",
        "{{\n",
        "    \"definition\": \"Quantity of oil i bought in month m\",\n",
        "    \"symbol\": \"buy_{{i,m}}\",\n",
        "    \"shape\": [\"I\",\"M\"]\n",
        "}}\n",
        "''')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "NZqMD0TPgoGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(name=\"TestAgent\", description=\"A test agent\", model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "# Call the model with a prompt\n",
        "response = agent.llm_call(prompt='''\n",
        "Assume the parameters are defined. Now generate a code accordingly and enclose it between \"=====\" lines. Only generate the gurobi code, and don't generate any other text. Here's an example:\n",
        "\n",
        "**input**:\n",
        "\n",
        "{{\n",
        "    \"definition\": \"Quantity of oil i bought in month m\",\n",
        "    \"symbol\": \"buy_{{i,m}}\",\n",
        "    \"shape\": [\"I\",\"M\"]\n",
        "}}\n",
        "''')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1-T_x4Tkqo7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variable_definition_prompt_templates = [\n",
        "    \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to write {solver} code for defining variables of the problem.\n",
        "\"\"\",\n",
        "    \"\"\"\n",
        "Here's a variable we need you to write the code for defining:\n",
        "\n",
        "-----\n",
        "{variable}\n",
        "-----\n",
        "\n",
        "Assume the parameters are defined. Now generate a code accordingly and enclose it between \"=====\" lines. Only generate the code, and don't generate any other text. Here's an example:\n",
        "\n",
        "**input**:\n",
        "\n",
        "{{\n",
        "    \"definition\": \"Quantity of oil i bought in month m\",\n",
        "    \"symbol\": \"buy_{{i,m}}\",\n",
        "    \"shape\": [\"I\",\"M\"]\n",
        "}}\n",
        "\n",
        "***output***:\n",
        "\n",
        "=====\n",
        "buy = model.addVars(I, M, vtype=gp.GRB.CONTINUOUS, name=\"buy\")\n",
        "=====\n",
        "\n",
        "\n",
        "- Note that the indices in symbol (what comes after _) are not a part of the variable name in code.\n",
        "- Use model.addVar instead of model.addVars if the variable is a scalar.\n",
        "\n",
        "\"\"\",\n",
        "]\n",
        "\n",
        "main_prompt_templates = {\n",
        "    \"constraint\": [\n",
        "        \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to write {solver} code for different constraints of the problem.\n",
        "\"\"\",\n",
        "        \"\"\"\n",
        "Here's a constraint we need you to write the code for, along with the list of related variables and parameters:\n",
        "\n",
        "-----\n",
        "{context}\n",
        "-----\n",
        "\n",
        "- Assume the parameters and variables are defined, and gurobipy is imported as gp. Now generate a code accordingly and enclose it between \"=====\" lines.\n",
        "- Only generate the code and the ===== lines, and don't generate any other text.\n",
        "- If the constraint requires changing a variable's integralilty, generate the code for changing the variable's integrality rather than defining the variable again.\n",
        "- If there is no code needed, just generate the comment line (using # ) enclosed in ===== lines explaining why.\n",
        "- Variables should become before parameters when defining inequality constraints in gurobipy (because of the gurobi parsing order syntax)\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "\n",
        "**input**:\n",
        "\n",
        "\n",
        "{{\n",
        "    \"description\": \"in month m, it is possible to store up to storageSize_{{m}} tons of each raw oil for use later.\",\n",
        "    \"formulation\": \"\\(storage_{{i,m}} \\leq storageSize, \\quad \\\\forall i, m\\)\",\n",
        "    \"related_variables\": [\n",
        "        {{\n",
        "            \"symbol\": \"storage_{{i,m}}\",\n",
        "            \"definition\": \"quantity of oil i stored in month m\",\n",
        "            \"shape\": [\n",
        "                \"I\",\n",
        "                \"M\"\n",
        "            ]\n",
        "        }}\n",
        "        ],\n",
        "    \"related_parameters\": [\n",
        "        {{\n",
        "            \"symbol\": \"storageSize_{{m}}\",\n",
        "            \"definition\": \"storage size available in month m\",\n",
        "            \"shape\": [\n",
        "                \"M\"\n",
        "            ]\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "\n",
        "***output***:\n",
        "\n",
        "=====\n",
        "# Add storage capacity constraints\n",
        "for i in range(I):\n",
        "    for m in range(M):\n",
        "        model.addConstr(storage[i, m] <= storageSize[m], name=\"storage_capacity\")\n",
        "=====\n",
        "\n",
        "Take a deep breath and approach this task methodically, step by step.\n",
        "\n",
        "\"\"\",\n",
        "    ],\n",
        "    \"objective\": [\n",
        "        \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to write {solver} code for the objective function of the problem.\n",
        "\"\"\",\n",
        "        \"\"\"\n",
        "Here's the objective function we need you to write the code for, along with the list of related variables and parameters:\n",
        "\n",
        "-----\n",
        "{context}\n",
        "-----\n",
        "\n",
        "Assume the parameters and variables are defined, and gurobipy is imported as gp. Now generate a code accordingly and enclose it between \"=====\" lines. Only generate the code and the =====s, and don't generate any other text. Here's an example:\n",
        "\n",
        "**input**:\n",
        "\n",
        "{{\n",
        "    \"description\": \"Maximize the total profit from selling goods\",\n",
        "    \"formulation\": \"Maximize \\(Z = \\sum_{{k=1}}^{{K}} \\sum_{{i=1}}^{{I}} (profit_k \\cdot x_{{k,i}} - storeCost \\cdot s_{{k,i}})\\)\",\n",
        "    \"related_variables\": [\n",
        "        {{\n",
        "            \"symbol\": \"x_{{k,i}}\",\n",
        "            \"definition\": \"quantity of product k produced in month i\",\n",
        "            \"shape\": [\n",
        "                \"K\",\n",
        "                \"I\"\n",
        "            ],\n",
        "            \"code\": \"x = model.addVars(K, I, vtype=gp.GRB.CONTINUOUS, name='x')\"\n",
        "        }},\n",
        "        {{\n",
        "            \"symbol\": \"s_{{k,i}}\",\n",
        "            \"definition\": \"quantity of product k stored in month i\",\n",
        "            \"shape\": [\n",
        "                \"K\",\n",
        "                \"I\"\n",
        "            ],\n",
        "            \"code\": \"s = model.addVars(K, I, vtype=gp.GRB.CONTINUOUS, name='s')\"\n",
        "        }}\n",
        "    ],\n",
        "    \"related_parameters\": [\n",
        "        {{\n",
        "            \"symbol\": \"profit_{{k}}\",\n",
        "            \"definition\": \"profit from selling product k\",\n",
        "            \"shape\": [\n",
        "                \"K\"\n",
        "            ]\n",
        "        }},\n",
        "        {{\n",
        "            \"symbol\": \"storeCost\",\n",
        "            \"definition\": \"price of storing one unit of product\",\n",
        "            \"shape\": []\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "\n",
        "\n",
        "***output***:\n",
        "\n",
        "=====\n",
        "# Set objective\n",
        "m.setObjective(gp.quicksum(profit[k] * x[k, i] - storeCost * s[k, i] for k in range(K) for i in range(I)), gp.GRB.MAXIMIZE)\n",
        "=====\n",
        "\n",
        "Take a deep breath and approach this task methodically, step by step.\n",
        "\n",
        "\"\"\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "debugging_prompt_templates = [\n",
        "    \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to debug the code for {target} of the problem.\n",
        "\"\"\",\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "When running a code snippet, an error happened. Here is the initial part of the code snippet for importing packages and defining the model:\n",
        "\n",
        "-----\n",
        "{prep_code}\n",
        "-----\n",
        "\n",
        "And here is the code for defining the related parameters and variables:\n",
        "\n",
        "-----\n",
        "{context}\n",
        "-----\n",
        "\n",
        "And the error happened when running this line:\n",
        "\n",
        "-----\n",
        "{error_line}\n",
        "-----\n",
        "\n",
        "and here is the error message:\n",
        "\n",
        "-----\n",
        "{error_message}\n",
        "-----\n",
        "\n",
        "We know that the import code is correct. First reason about the source of the error. Then, if the code is correct and the problem is likely to be in the formulation, generate a json in this format (the reason is why you think the problem is in the formulation):\n",
        "\n",
        "{{\n",
        "    \"status\": \"correct\",\n",
        "    \"reason\": ?\n",
        "}}\n",
        "\n",
        "Otherwise, fix the code and generate a json file with the following format:\n",
        "\n",
        "{{\n",
        "    \"status\": \"fixed\",\n",
        "    \"fixed_code\": ?\n",
        "}}\n",
        "\n",
        "\n",
        "- Note that the fixed code should be the fixed version of the original error line, not the whole code snippet.\n",
        "- Do not generate any text after the json file. All the imports and model definition are already done, and you should only generate the fixed code to be replaced with the original error line.\n",
        "\n",
        "\"\"\",\n",
        "]\n",
        "\n",
        "debugging_refined_template_target = \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to debug the code for of the problem.\n",
        "\n",
        "When running the following code snipper, an error happened:\n",
        "\n",
        "-----\n",
        "{prep_code}\n",
        "\n",
        "{error_line}\n",
        "-----\n",
        "\n",
        "and here is the error message:\n",
        "\n",
        "-----\n",
        "{error_message}\n",
        "-----\n",
        "\n",
        "We know that the code for importing packages and defining parameters and variables is correct, and the error is because of the this last part which is for modeling the {target}:\n",
        "\n",
        "-----\n",
        "{error_line}\n",
        "-----\n",
        "\n",
        "First reason about the source of the error. Then, if the code is correct and the problem is likely to be in the formulation, generate a json in this format (the reason is why you think the problem is in the formulation):\n",
        "\n",
        "{{\n",
        "    \"status\": \"correct\",\n",
        "    \"reason\": \"A string explaining why you think the problem is in the formulation\"\n",
        "}}\n",
        "\n",
        "otherwise, fix the last part code and generate a json file with the following format:\n",
        "\n",
        "{{\n",
        "    \"status\": \"fixed\",\n",
        "    \"fixed_code\": \"A sting representing the fixed {target} modeling code to be replaced with the last part code\"\n",
        "}}\n",
        "\n",
        "- Note that the fixed code should be the fixed version of the last part code, not the whole code snippet. Only fix the part that is for modeling the {target}.\n",
        "- Do not generate any text after the json file.\n",
        "- Variables should become before parameters when defining inequality constraints in gurobipy (because of the gurobi parsing order syntax)\n",
        "\n",
        "Take a deep breath and solve the problem step by step.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "debugging_refined_template_variable = \"\"\"\n",
        "You're an expert programmer in a team of optimization experts. The goal of the team is to solve an optimization problem. Your responsibility is to debug the code for of the problem.\n",
        "\n",
        "When running the following code snipper, an error happened:\n",
        "\n",
        "-----\n",
        "{prep_code}\n",
        "\n",
        "{error_line}\n",
        "-----\n",
        "\n",
        "and here is the error message:\n",
        "\n",
        "-----\n",
        "{error_message}\n",
        "-----\n",
        "\n",
        "We know that the code for importing packages and defining parameters and variables is correct, and the error is because of the this last part which is for modeling the {target}:\n",
        "\n",
        "-----\n",
        "{error_line}\n",
        "-----\n",
        "\n",
        "First reason about the source of the error. Then, if the code is correct and the problem is likely to be in the formulation, generate a json in this format (the reason is why you think the problem is in the formulation):\n",
        "\n",
        "{{\n",
        "    \"status\": \"correct\",\n",
        "    \"reason\": \"A string explaining why you think the problem is in the formulation\"\n",
        "}}\n",
        "\n",
        "otherwise, fix the last part code and generate a json file with the following format:\n",
        "\n",
        "{{\n",
        "    \"status\": \"fixed\",\n",
        "    \"fixed_code\": \"A sting representing the fixed {target} modeling code to be replaced with the last part code\"\n",
        "}}\n",
        "\n",
        "- Note that the fixed code should be the fixed version of the last part code, not the whole code snippet. Only fix the part that is for defining the {target}.\n",
        "- Do not generate any text after the json file.\n",
        "- Variables should become before parameters when defining inequality constraints in gurobipy (because of the gurobi parsing order syntax)\n",
        "\n",
        "Take a deep breath and solve the problem step by step.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "vhHzuzvZYlmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Programmer(Agent1):\n",
        "    def __init__(\n",
        "        self, model_name=\"gemma2\", solver=\"gurobipy\", debugger_on=True, **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            name=\"Programmer\",\n",
        "            description=\"This is a mathematical programmer agent that is an expert in writing, modifying, and debugging code for optimization problems from the mathematical formulation of the problem. This agent should be called first when a bug or error happens in the code.\",\n",
        "            model_name=model_name,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._debugger_on = debugger_on\n",
        "        self.solver = solver\n",
        "\n",
        "    def generate_reply(self, task: str, state: Dict, sender: Agent) -> Tuple[str, Dict]:\n",
        "        # add some lines and characters around it to make the input interface nicer\n",
        "        print(\"- Programmer agent is called!\")\n",
        "        print()\n",
        "\n",
        "        if state[\"solution_status\"] == \"runtime_error\":\n",
        "            # Enter debugging mode\n",
        "            bogus_item = None\n",
        "            for target in [\"constraint\", \"objective\", \"variables\"]:\n",
        "                for item in state[target]:\n",
        "                    if not item[\"status\"] in [\"coded\", \"formulated\", \"runtime_error\"]:\n",
        "                        # raise Exception(\n",
        "                        #     f\"{target} {item} inconsistency in state! \\n {json.dumps(state, indent=4)}\"\n",
        "                        # )\n",
        "                        print(\n",
        "                            f\"{target} {item} inconsistency in state! \\n {json.dumps(state, indent=4)}\"\n",
        "                        )\n",
        "                    if item[\"status\"] == \"runtime_error\":\n",
        "                        bogus_item = item\n",
        "                        break\n",
        "\n",
        "            if not bogus_item:\n",
        "                raise Exception(\n",
        "                    \"No runtime error in state!\", json.dumps(state, indent=4)\n",
        "                )\n",
        "\n",
        "            return self._debug_code(state=state)\n",
        "\n",
        "        elif state[\"solution_status\"] is None:\n",
        "            # Enter coding mode\n",
        "            return self._generate_code_from_formulation(state=state)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Invalid solver_output_status {state['solver_output_status']}!\"\n",
        "            )\n",
        "\n",
        "\n",
        "    def _debug_code(self, state: Dict) -> Tuple[str, Dict]:\n",
        "        if not self._debugger_on:\n",
        "            raise Exception(\"Debugger is off. Execution failed\")\n",
        "\n",
        "        error_line = None\n",
        "        bogus_context = None\n",
        "\n",
        "        for target in [\"constraint\", \"objective\", \"variables\"]:\n",
        "            for item in state[target]:\n",
        "                if item[\"status\"] == \"runtime_error\":\n",
        "                    bogus_context = item\n",
        "\n",
        "        context = {}\n",
        "        prep_code = state[\"prep_code\"]\n",
        "\n",
        "        if \"description\" in bogus_context:\n",
        "            error_line = bogus_context[\"code\"]\n",
        "            error_message = state[\"error_message\"]\n",
        "            for parameter in state[\"parameters\"]:\n",
        "                if parameter[\"symbol\"] in bogus_context[\"related_parameters\"]:\n",
        "                    prep_code += parameter[\"code\"] + \"\\n\"\n",
        "\n",
        "            for variable in state[\"variables\"]:\n",
        "                if variable[\"symbol\"] in bogus_context[\"related_variables\"]:\n",
        "                    if not \"code\" in variable:\n",
        "                        raise Exception(f\"Variable {variable} is not coded yet!\")\n",
        "\n",
        "                    prep_code += variable[\"code\"] + \"\\n\"\n",
        "            prompt = debugging_refined_template_target.format(\n",
        "                target=target,\n",
        "                prep_code=prep_code,\n",
        "                error_line=error_line,\n",
        "                error_message=error_message,\n",
        "            )\n",
        "\n",
        "        elif \"definition\" in bogus_context:\n",
        "            error_line = bogus_context[\"code\"]\n",
        "            error_message = state[\"error_message\"]\n",
        "\n",
        "            prompt = debugging_refined_template_variable.format(\n",
        "                target=target,\n",
        "                prep_code=prep_code,\n",
        "                error_line=error_line,\n",
        "                error_message=error_message,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Invalid bogus_context {bogus_context}! \\n {json.dumps(state, indent=4)}\"\n",
        "            )\n",
        "\n",
        "        cnt = 3\n",
        "        while cnt > 0:\n",
        "            cnt -= 1\n",
        "            try:\n",
        "                print(\"%^%^%\")\n",
        "                print(prompt)\n",
        "                response = self.llm_call(prompt=prompt, seed=cnt)\n",
        "                print(response)\n",
        "                print(\"%^%^%\")\n",
        "                response = response[response.find(\"```json\") + 7 :]\n",
        "                response = response[: response.rfind(\"```\")]\n",
        "\n",
        "                update = json.loads(response)\n",
        "\n",
        "                if update[\"status\"] == \"correct\":\n",
        "                    bogus_context[\"status\"] = \"formulation_error\"\n",
        "                    return update[\"reason\"], state\n",
        "                elif update[\"status\"] == \"fixed\":\n",
        "                    bogus_context[\"status\"] = \"coded\"\n",
        "                    bogus_context[\"code\"] = update[\"fixed_code\"]\n",
        "                    return \"The code is fixed! Try evaluating it again.\", state\n",
        "                else:\n",
        "                    raise Exception(f\"Invalid status {update['status']}!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(f\"Invalid json format {response}! Try again ...\")\n",
        "\n",
        "\n",
        "\n",
        "    def _generate_code_from_formulation(self, state: Dict) -> Tuple[str, Dict]:\n",
        "        for variable in state[\"variables\"]:\n",
        "            print(f\"Programming variable {variable['symbol']} ...\")\n",
        "\n",
        "            if variable[\"status\"] == \"not_formulated\":\n",
        "                raise Exception(f\"Variable {variable} is not formulated yet!\")\n",
        "\n",
        "            elif variable[\"status\"] == \"formulated\":\n",
        "                context = {}\n",
        "                context[\"definition\"] = variable[\"definition\"]\n",
        "                context[\"symbol\"] = variable[\"symbol\"]\n",
        "                context[\"shape\"] = variable[\"shape\"]\n",
        "\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": variable_definition_prompt_templates[0].format(\n",
        "                            solver=self.solver\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": variable_definition_prompt_templates[1].format(\n",
        "                            variable=context,\n",
        "                        ),\n",
        "                    },\n",
        "                ]\n",
        "\n",
        "                cnt = 3\n",
        "                while cnt > 0:\n",
        "                    try:\n",
        "                        response = self.llm_call(messages=messages, seed=cnt)\n",
        "                        print(response)\n",
        "                        code = [\n",
        "                            r.strip()\n",
        "                            for r in response.split(\"=====\")\n",
        "                            if len(r.strip()) > 2\n",
        "                        ][-1]\n",
        "\n",
        "                        code = code.strip()\n",
        "                        while code[0] == \"=\":\n",
        "                            code = code[1:].strip()\n",
        "                        while code[-1] == \"=\":\n",
        "                            code = code[:-1].strip()\n",
        "\n",
        "                        if len(code) < 2:\n",
        "                            raise Exception(f\"Invalid code {code}!\")\n",
        "\n",
        "                        code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                        variable[\"code\"] = code\n",
        "                        variable[\"status\"] = \"coded\"\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        cnt -= 1\n",
        "                        import traceback\n",
        "\n",
        "                        print(traceback.print_exc())\n",
        "                        print(messages[1][\"content\"])\n",
        "                        print(response)\n",
        "                        print(e)\n",
        "                        print(f\"Invalid response {response}! Try again ...\")\n",
        "\n",
        "                        if cnt == 0:\n",
        "                            raise e\n",
        "\n",
        "            elif variable[\"status\"] == \"coded\":\n",
        "                pass\n",
        "\n",
        "        for target in [\"constraint\", \"objective\"]:\n",
        "            for item in state[target]:\n",
        "                print(f\"Programming {target} ...\")\n",
        "                if item[\"status\"] == \"not_formulated\":\n",
        "                    raise Exception(f\"{target} {item} is not formulated yet!\")\n",
        "\n",
        "                elif item[\"status\"] == \"formulated\":\n",
        "                    context = {}\n",
        "                    context[\"description\"] = item[\"description\"]\n",
        "                    context[\"formulation\"] = item[\"formulation\"]\n",
        "                    context[\"related_variables\"] = []\n",
        "                    context[\"related_parameters\"] = []\n",
        "\n",
        "                    for parameter in state[\"parameters\"]:\n",
        "                        if parameter[\"symbol\"] in item[\"related_parameters\"]:\n",
        "                            context[\"related_parameters\"].append(parameter)\n",
        "\n",
        "                    for variable in state[\"variables\"]:\n",
        "                        if variable[\"symbol\"] in item[\"related_variables\"]:\n",
        "                            if not \"code\" in variable:\n",
        "                                raise Exception(\n",
        "                                    f\"Variable {variable} is not coded yet!\"\n",
        "                                )\n",
        "                            context[\"related_variables\"].append(\n",
        "                                {\n",
        "                                    \"symbol\": variable[\"symbol\"],\n",
        "                                    \"definition\": variable[\"definition\"],\n",
        "                                    \"shape\": variable[\"shape\"],\n",
        "                                    \"code\": variable[\"code\"],\n",
        "                                }\n",
        "                            )\n",
        "\n",
        "                    messages = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": main_prompt_templates[target][0].format(\n",
        "                                solver=self.solver\n",
        "                            ),\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": main_prompt_templates[target][1].format(\n",
        "                                context=json.dumps(context, indent=4),\n",
        "                            ),\n",
        "                        },\n",
        "                    ]\n",
        "\n",
        "                    cnt = 3\n",
        "\n",
        "                    while cnt > 0:\n",
        "                        try:\n",
        "                            response = self.llm_call(messages=messages, seed=cnt)\n",
        "                            print(response)\n",
        "                            code = [\n",
        "                                r.strip()\n",
        "                                for r in response.split(\"=====\")\n",
        "                                if len(r.strip()) > 2\n",
        "                            ][-1]\n",
        "\n",
        "                            code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                            item[\"code\"] = code\n",
        "                            item[\"status\"] = \"coded\"\n",
        "                            break\n",
        "                        except Exception as e:\n",
        "                            import traceback\n",
        "\n",
        "                            print(traceback.print_exc())\n",
        "                            print(messages[1][\"content\"])\n",
        "                            print(response)\n",
        "                            cnt -= 1\n",
        "                            if cnt == 0:\n",
        "                                raise e\n",
        "\n",
        "                else:\n",
        "                    raise Exception(f\"{target} {item} is not formulated yet!\")\n",
        "\n",
        "        return \"Coding Done! Now we can evaluate the code!\", state\n"
      ],
      "metadata": {
        "id": "d8MjE1uuZEC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}