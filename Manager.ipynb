{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1r7az0npRQX4OcKSy8ymF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushc2004/LLM/blob/main/Manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxnPmpN_4z4u",
        "outputId": "094c589f-2aef-4f9e-d71f-7d4556fea369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kushc2004/LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppsej0-3C8D1",
        "outputId": "e7ab9e27-33de-47c9-ba41-b493e1daac3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 18 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (18/18), 41.66 KiB | 2.45 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from /content/agent.Agent import Agent\n",
        "import json\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "import torch\n",
        "\n",
        "class GroupChatManager(Agent):\n",
        "    def __init__(\n",
        "        self, model_name: str, agents: [Agent], max_rounds: int = 12, **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            name=\"GroupChatManager\",\n",
        "            description=\"This is a manager agent that chooses which agent to work on the problem next and organizes the conversation within its team.\",\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.agents = agents\n",
        "        self.conversation_state = {\n",
        "            \"round\": 0,\n",
        "        }\n",
        "        self.max_rounds = max_rounds\n",
        "        self.history = []\n",
        "        self.prompt_template = \"\"\"\n",
        "\n",
        "You're a manager in a team of optimization experts. The goal of the team is to solve an optimization problem. Your task is to choose the next expert to work on the problem based on the current situation.\n",
        "- The user has already given us the problem description, the objective function, and the parameters. Only call the user proxy if there is a problem or something ambiguous or missing.\n",
        "\n",
        "Here's the list of agents in your team:\n",
        "-----\n",
        "{agents}\n",
        "-----\n",
        "\n",
        "And here's the history of the conversation so far:\n",
        "-----\n",
        "{history}\n",
        "-----\n",
        "\n",
        "\n",
        "Considering the history, if you think the problem is solved, type DONE. Otherwise, generate a json file with the following format:\n",
        "{{\n",
        "    \"agent_name\": \"Name of the agent you want to call next\",\n",
        "    \"task\": \"The task you want the agent to carry out\"\n",
        "}}\n",
        "\n",
        "to identify the next agent to work on the problem, and also the task it has to carry out.\n",
        "- If there is a runtime error, ask the the programmer agent to fix it.\n",
        "- Only generate the json file, and don't generate any other text.\n",
        "- If the latest message in history says that the code is fixed, ask the evaluator agent to evaluate the code!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Load the Llama model and tokenizer\n",
        "        self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "        self.model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    def llm_call(self, prompt: str, seed: int) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(**inputs, max_length=512)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    def solve(self, state: Dict) -> (str, Dict):\n",
        "        self.history = []\n",
        "\n",
        "        while True:\n",
        "            if self.conversation_state[\"round\"] >= self.max_rounds:\n",
        "                return \"The problem is not solved.\", state\n",
        "\n",
        "            print(\"=\" * 20)\n",
        "            print(\"=\" * 20)\n",
        "            print(\"Round\", self.conversation_state[\"round\"])\n",
        "\n",
        "            agents_list = \"\".join(\n",
        "                [\n",
        "                    \"-\" + agent.name + \": \" + agent.description + \"\\n\"\n",
        "                    for agent in self.agents\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            prompt = self.prompt_template.format(\n",
        "                agents=agents_list,\n",
        "                history=\"\\n\".join([json.dumps(item[0]) for item in self.history]),\n",
        "            )\n",
        "\n",
        "            cnt = 3\n",
        "            while True and cnt > 0:\n",
        "                try:\n",
        "                    response = self.llm_call(prompt=prompt, seed=cnt)\n",
        "\n",
        "                    decision = response.strip()\n",
        "                    if \"```json\" in decision:\n",
        "                        decision = decision.split(\"```json\")[1].split(\"```\")[0]\n",
        "                    decision = decision.replace(\"\\\\\", \"\")\n",
        "\n",
        "                    if decision == \"DONE\":\n",
        "                        print(\"DONE\")\n",
        "                        return \"The problem is solved.\", state\n",
        "                    decision = json.loads(decision)\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(response)\n",
        "                    print(e)\n",
        "                    cnt -= 1\n",
        "\n",
        "                    print(\"Invalid decision. Trying again ...\")\n",
        "                    if cnt == 0:\n",
        "                        import traceback\n",
        "\n",
        "                        err = traceback.format_exc()\n",
        "                        print(err)\n",
        "\n",
        "            print(\n",
        "                \"---- History:\\n\",\n",
        "                \"\\n\".join([json.dumps(item[0]) for item in self.history]),\n",
        "            )\n",
        "\n",
        "            print(f\"\\n---- Decision:||{decision}||\\n\")\n",
        "\n",
        "            if not decision[\"agent_name\"] in [agent.name for agent in self.agents]:\n",
        "                raise ValueError(\n",
        "                    f\"Decision {decision} is not a valid agent name. Please choose from {self.agents}\"\n",
        "                )\n",
        "            else:\n",
        "                agent = [\n",
        "                    agent\n",
        "                    for agent in self.agents\n",
        "                    if agent.name == decision[\"agent_name\"]\n",
        "                ][0]\n",
        "\n",
        "                message, new_state = agent.generate_reply(\n",
        "                    task=decision[\"task\"],\n",
        "                    state=state,\n",
        "                    sender=self,\n",
        "                )\n",
        "\n",
        "                with open(\n",
        "                    f\"{state['log_folder']}/log_{self.conversation_state['round']}.json\",\n",
        "                    \"w\",\n",
        "                ) as f:\n",
        "                    json.dump(state, f, indent=4)\n",
        "\n",
        "                state = new_state\n",
        "\n",
        "                decision[\"result\"] = message\n",
        "                self.history.append((decision, state))\n",
        "\n",
        "                with open(state[\"log_folder\"] + \"/selection_log.json\", \"w\") as f:\n",
        "                    json.dump([d for (d, s) in self.history], f, indent=4)\n",
        "\n",
        "                if \"code\" in state:\n",
        "                    with open(state[\"log_folder\"] + \"/code.py\", \"w\") as f:\n",
        "                        f.write(state[\"code\"])\n",
        "\n",
        "                self.conversation_state[\"round\"] += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "_9tXRa1u-Gqz",
        "outputId": "b90ece40-94ae-4c50-e4ad-aab62e34f93c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'agents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d9e01542f971>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agents'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}